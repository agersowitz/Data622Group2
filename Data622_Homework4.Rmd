---
title: "Data622 - Group2 - Homework4"
author: "Zachary Palmore, Kevin Potter, Amit Kapoor, Adam Gersowitz, Paul Perez"
date: "10/21/2021"
output:
  html_document:
    fig_width: 15
    highlight: pygments
    number_sections: no
    theme: flatly
    toc: yes
    toc_float: yes
  pdf_document:
    latex_engine: xelatex
    toc: yes
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, error=FALSE, warning=FALSE, message=FALSE, fig.align="center", fig.width = 10)
```


# Overview

In this project, we analyze a real-life mental health dataset to provide context around suicide prediction given a variety of unidentifiable demographic data. Our goals are to understand the variables relationships, identify those variables that influence our target, and develop models that can predict a patient's risk of suicide. 

# Approach
We will first perform exploratory data analysis (EDA) on the dataset to inform our analysis and build better models. Methods include Clustering, Principal Component Analysis, Gradient Boosting, and Support Vector Machines. This EDA step is crucial to understanding variables' relationships and identifying which variables influence our target. 

Once we understand the data, we prepare it for modeling. This includes partitioning the data with a 75-25 train-test split, performing necessary imputations, relevant centering and scaling, and more as outlined in our data exploration and preparation sections. When building our models we focus on using methods that produce real-world accuracy. For this reason, we attempt to select the best generalizable model with accuracy as our primary indicator during model evaluation.  

```{r libraries, include=FALSE, warning=FALSE, message=FALSE}
# Libraries
library(summarytools)
library(tidyverse)
library(DataExplorer)
library(reshape2)
library(mice)
library(caret)
library(MASS)
library(e1071)
library(tree)
library(corrplot)
library(kableExtra)
library(htmltools)
library(readxl)
library(psych)
library(xgboost)
library(ParBayesianOptimization)
library(factoextra)
set.seed(622)
```

# Data Exploration

The dataset with its column IDs, variable names, and variables descriptions are provided below for reference.

Columns | Variable | Description  
---|---|-----  
C |  Sex | Male-1, Female-2  
D | Race | White-1, African American-2, Hispanic-3, Asian-4, Native American-5, Other or missing data -6  
E - W | ADHD self-report scale |  Never-0, rarely-1, sometimes-2, often-3, very often-4  
X - AM | Mood disorder questions |  No-0, yes-1; question 3: no problem-0, minor-1, moderate-2, serious-3  
AN - AS | Individual substances misuse |   no use-0, use-1, abuse-2, dependence-3   
AT | Court Order |   No-0, Yes-1  
AU | Education |  1-12 grade, 13+ college  
AV | History of Violence |  No-0, Yes-1  
AW | Disorderly Conduct |  No-0, Yes-1  
AX | Suicide attempt |  No-0, Yes-1  
AY | Abuse Hx |  No-0, Physical (P)-1, Sexual (S)-2, Emotional (E)-3, P&S-4, P&E-5, S&E-6, P&S&E-7  
AZ | Non-substance-related Dx |  0 - none; 1 - one; 2 - More than one  
BA | Substance-related Dx |  0 - none; 1 - one Substance-related; 2 - two; 3 - three or more  
BB | Psychiatric Meds |  0 - none; 1 - one psychotropic med; 2 - more than one psychotropic med  

Notice how the data is grouped with ADHD, Mood disorders, and Individual Substance misuse present across a range of columns. These groups are reviewed throughout the exploration process and new features are generated to attempt to improve model performance. 

## Data Characteristics

```{r data}
# read data
adhd_data <- read_excel("ADHD_data.xlsx", sheet = "Data") %>% na_if("") %>% dplyr::select(-1)
#columns <- list(dimnames(adhd_data)[2])
#df <- adhd_data[,2:53]
adhd_data[,2:53] <- lapply(adhd_data[,2:53], factor)
adhd_data.dims <- dim(adhd_data)
adhd_data.dims[[2]]
```

The data contains `r adhd_data.dims[[1]]` observations of `r adhd_data.dims[[2]]` variables. We import the data from a remote repository and find that 51 of the variables should be of the factor data type given clear levels in their distributions. As is, these variables are interpreted as character strings. This will need to be converted for realistic results. The remaining variables can be numeric for our purposes. 

We review one grouped variable set, known as mood disorders (MD), to show what we're working with. These contain a series of associated questions (Q1-Q3) with Q1 containing parts 'a' through 'm.'

```{r}
adhd_data[,c(23:37)]
```
Each part of Q1 'a' through 'm' corresponds with a specific question related to mood disorders for a single patient. In our feature engineering, it may be useful to tally these responses for a more holistic perspective of the patient's overall mood. We repeat this for the other groups to get an sense of the patient well-being which should provide insight into their risk of suicide.   

```{r, cat-bar, fig.length =20, fig.width=10}
# select categorical columns
cat_cols <- dimnames(adhd_data[,2:53])[[2]]
adhd_fact <-  adhd_data[cat_cols]
# long format
adhd_factm <- melt(adhd_fact, measure.vars = cat_cols, variable.name = 'metric', value.name = 'value')
# plot categorical columns
ggplot(adhd_factm, aes(x = value)) + 
  geom_bar(aes(fill = metric)) + 
  facet_wrap( ~ metric, nrow = 5L, scales = 'free') + coord_flip() + 
  theme(legend.position = "none")
```

## Data summary


```{r adhd_data_summary}
dfSummary(adhd_data, style = 'grid', graph.col = FALSE)
```

## Coorelation

Next we will see the correlation among ADHD questions and MD questions. As we can deduce from below 2 correlation plots, ADHD questions are highly correlated and MD questions comparatively shows moderate correlation.

```{r corr-adhds}
adhds <- sapply(adhd_data[,c(4:21)], as.numeric) %>% cor()
corrplot::corrplot(adhds, method="number")
```

```{r corr-mds}
mds <- sapply(adhd_data[,c(23:37)], as.numeric) %>% cor()
corrplot::corrplot(mds, method="number")
```



# Data Preparation

## Factor Analysis

Like PCA, Factor Analysis too, reduces larger number of variables into smaller number of variables, called latent variables.It is used to identify underlying factors that explain the correlation among set of variables. Factor analysis is a great tool for treating multivariate questionnaire studies.

For ADHD questions, test of the hypothesis that 3 factors are sufficient. The chi square statistic is 197.3 on 102 degrees of freedom. The p-value is 0.0000000476. We have used regression factor scores here as they predict the location of each individual on the factor.



```{r adhd-ques-fa}
adhd_ques_fa <- factanal(sapply(adhd_data[,c(4:21)], as.numeric), 
                         factors = 3, 
                         rotation = "promax", 
                         scores = "regression")
adhd_ques_fa
```



```{r adhd-fa-diag}
fa.diagram(adhd_ques_fa$loadings)
```


For MD questions we could see that 1st MD question has multiple sub questions as compared to 2nd and 3rd question. Now for these set of MD questions too, we will apply similar factor analysis as of ADHD questions. Test of the hypothesis that 3 factors are sufficient. The chi square statistic is 88.82 on 63 degrees of freedom. The p-value is 0.0178.



```{r md-ques-fa}
md_ques_fa <- factanal(sapply(adhd_data[,c(23:37)], as.numeric), 
                         factors = 3, 
                         rotation = "promax", 
                         scores = "regression")
md_ques_fa
```



```{r mdfa-diag}
fa.diagram(md_ques_fa$loadings)
```

In the next step we will remove all ADHD Question columns, ADHD Total, MD questions columns and MD TOTAL columns. Then we will add the new factors found above for ADHD and MD questions.


```{r}
# ADHD question scores dataframe
adhd_ques_fa <- as.data.frame(adhd_ques_fa$scores) 
names(adhd_ques_fa) <- c('ADHD_FACT1','ADHD_FACT2','ADHD_FACT3')

# MD questions scores dataframe
md_ques_fa <- as.data.frame(md_ques_fa$scores)
names(md_ques_fa) <- c('MD_FACT1','MD_FACT2','MD_FACT3')

# remove ADHD and MD columns
adhd_newdata <- adhd_data %>% dplyr::select(-c(starts_with('ADHD Q'), starts_with('MD Q')))

# Add new factor columns created
adhd_newdata <- cbind(adhd_newdata, adhd_ques_fa, md_ques_fa)
```

Here is glimpse of new set of data.

```{r}
head(adhd_newdata)
```




## Handling missing values


```{r}
# plot missing values
plot_missing(adhd_newdata)
```

We can see from this chart that `Psych meds.` contributes to 67.43% of missing data which is maximum among all missing data in other columns. We will remove this column before imputation. We then impute values using MICE (Multivariate Imputation by Chained Equations) for columns having missing values.

```{r}
# rename columns to apply mice
adhd_newdata <- adhd_newdata %>% 
  rename('ADHD_Total'='ADHD Total',
         'MD_Total'='MD TOTAL',
         'Sedative_hypnotics'='Sedative-hypnotics', 
         'Court_order' = 'Court order', 
         'Hx_of_Violence'='Hx of Violence', 
         'Disorderly_Conduct'='Disorderly Conduct', 
         'Non_subst_Dx'='Non-subst Dx',
         'Subst_Dx'='Subst Dx', 
         'Psych_meds'='Psych meds.') %>% 
  dplyr::select(-Psych_meds)
```



```{r impute}
# select columns with non missing values
temp <- adhd_newdata %>% dplyr::select(c(starts_with('ADHD_'), starts_with('MD_'), 'Race', 'Sex', 'Age'))

# impute predictors using mice
adhd_impute <- adhd_newdata %>% dplyr::select(-c(starts_with('ADHD_'), starts_with('MD_'), 'Race', 'Sex', 'Age'))
adhd_impute <- complete(mice(data=adhd_impute, print=FALSE))
summary(adhd_impute)
```



```{r}
# Merged the imputed dataframe with temp
adhd_newdata <- cbind(adhd_impute, temp)
head(adhd_newdata)
```



```{r cat-missing}
# Filter out 
#adhd_data <- adhd_data %>% filter(!is.na(Alcohol) &
#                                  !is.na(THC) &
#                                  !is.na(Cocaine) &
#                                  !is.na(Stimulants) &
#                                  !is.na(`Sedative-hypnotics`) &
#                                  !is.na(Opioids) &
#                                  !is.na(`Court order`) &
#                                  !is.na(Education) &
#                                  !is.na(`Hx of Violence`) &
#                                  !is.na(`Disorderly Conduct`) &
#                                  !is.na(Suicide) &
#                                  !is.na(Abuse) &
#                                  !is.na(`Non-subst Dx`) &
#                                  !is.na(`Subst Dx`) &
#                                  !is.na(`Psych meds.`))
```



```{r num-missing}
# impute numeric predictors using mice
#adhd_data <- complete(mice(data=adhd_data[,:53], method="pmm", print=FALSE))
```




## Preprocess using transformation

In this transformation, we would first use dummyVars to create dummy variables for categorical features. Next we use center and scaling transformation.

```{r transform}
set.seed(622)

# create dummy variables for categorical features
adhd_dummy <- dummyVars(Suicide ~ ., data = adhd_newdata) 
adhd_dummy <- predict(adhd_dummy, newdata=adhd_newdata)

# center and scaling
adhd_transformed <- adhd_dummy %>% 
  preProcess(c("center", "scale")) %>% 
  predict(adhd_dummy) %>% 
  as.data.frame()

# add Suicide column
adhd_transformed$Suicide <- adhd_newdata$Suicide

head(adhd_transformed)
```


## Training and Test Partition

In this step for data preparation we will partition the training dataset in training and validation sets using `createDataPartition` method from `caret` package. We will reserve 75% for training and rest 25% for validation purpose.

```{r partition}
set.seed(622)
partition <- createDataPartition(adhd_data$Suicide, p=0.75, list = FALSE)
training <- adhd_data[partition,]
testing <- adhd_data[-partition,]
# training/validation partition for independent variables
#X.train <- ld.clean[partition, ] %>% dplyr::select(-Loan_Status)
#X.test <- ld.clean[-partition, ] %>% dplyr::select(-Loan_Status)
# training/validation partition for dependent variable Loan_Status
#y.train <- ld.clean$Loan_Status[partition]
#y.test <- ld.clean$Loan_Status[-partition]
```

# Clustering Models
## K-means
## Hierarchical

# Principal Component Analysis
## Individual Substance Misuse

Principal Compenent Analysis (PCA) is one way for which we can rudce the dimensionality of a data set which would help increase the interpretability of the data while minimizing information loss. We're going to perform PCA for the 3 groups below while using scree plots to determine the numberof PCA's to keep. The Scree plot will display the eigenvalues in a downward curve, and order them from largest to smallest.

Groups: 
- All ADHD Questions
- All MD Questions
- All Remaining Features

```{r}
# create subset of ADHD Questions for PCA
adhd_ques_pca <- sapply(adhd_data[,c(4:21)], as.numeric)

# create subset of MD Questions for PCA
md_ques_pca <- sapply(adhd_data[,c(23:37)], as.numeric)

# create dataset of remaining features for PCA
rf_ques_pca <- sapply(adhd_data[,c(39:53)], as.numeric)
```


### ADHD
```{r}
pca_adhd <- prcomp(adhd_ques_pca, scale. = TRUE)
summary(pca_adhd)
```


```{r}
plot(pca_adhd, type = 'l', col = 'blue')
```

```{r}
biplot(pca_adhd, choices = c(1,2), scale = 0)
```

```{r}
biplot(pca_adhd, choices = c(2,3), scale = 0)
```

```{r}
biplot(pca_adhd, choices = c(1,3), scale = 0)
```


### MD

```{r}
pca_md <- prcomp(md_ques_pca, scale. = TRUE)
summary(pca_md)
```

```{r}
plot(pca_md, type = 'l', col = 'dark green')
```

```{r}
biplot(pca_md, choices = c(1,2), scale = 0)
```

```{r}
biplot(pca_md, choices = c(2,3), scale = 0)
```

```{r}
biplot(pca_md, choices = c(1,3), scale = 0)
```


```{r}
adhd_ques_pca <- sapply(adhd_data[,c(4:21)], as.numeric)
pca_adhd <- prcomp(adhd_ques_pca, scale. = TRUE, center=TRUE)
cor(adhd_ques_pca, pca_adhd$x[,1:10]) %>% 
  kableExtra::kbl(booktabs = T, caption ="ADHD Correlation Statistics") %>% 
  kable_styling(latex_options = c("striped"), full_width = F) 
summary(pca_adhd)
fviz_eig(pca_adhd)
#top 10 contributors to the dimension of PC1 and PC2
fviz_contrib(pca_adhd, choice = "var", axes = c(1,2), top = 15)
fviz_pca_var(pca_adhd,
             col.var ="contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             ,axes=c(1,2)
             )
#top 10 contributors to the dimension of PC1 and PC3
fviz_contrib(pca_adhd, choice = "var", axes = c(1,3), top = 15)
fviz_pca_var(pca_adhd,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             ,axes=c(1,3)
             )
#top 10 contributors to the dimension of PC2 and PC3
fviz_contrib(pca_adhd, choice = "var", axes = c(2,3), top = 15)
fviz_pca_var(pca_adhd,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             ,axes=c(2,3)
             )
```




### Remaining Features

```{r}
#pca_rf <- prcomp(rf_ques_pca, scale. = TRUE)
#summary(pca_rf)
```


# Gradient Boosting: Suicide

Assume you are modeling whether a patient attempted suicide (column AX). This is a binary
target variable. Please use Gradient Boosting to predict whether a patient attempts suicides.
Please use whatever boosting approach you deem appropriate. But please be sure to walk us
through your steps.

We remove the rows null values in the target column and drop the Non-subset Dx column because it had a lot of nulls as well. XGBoost needs data to be in a matrix so we convert the dataframes to numeric matricies. 

```{r}
gb__train <-subset(training[complete.cases(training$Suicide), ], select= -`Non-subst Dx`)
gb__test <-subset(testing[complete.cases(testing$Suicide), ], select= -`Non-subst Dx`)
y_label_tr <- as.matrix(gb__train$Suicide)
y_label_test <- as.matrix(gb__test$Suicide)
gb__train <- sapply(subset(gb__train, select = -Suicide), as.numeric)
gb_test <- sapply(subset(gb__test, select = -Suicide), as.numeric)
```


## CV Split

We split the data into three folds for cross validation to imrove the ability of the model to generailze and helpwit h overfitting. We create a function to help with parameter tunning and make use of the bayesOpt package.  

https://cran.r-project.org/web/packages/ParBayesianOptimization/vignettes/tuningHyperparameters.html

```{r}
Folds <- list(
    Fold1 = as.integer(seq(1,nrow(gb__train),by = 3))
  , Fold2 = as.integer(seq(2,nrow(gb__train),by = 3))
  , Fold3 = as.integer(seq(3,nrow(gb__train),by = 3))
)

scoringFunction <- function(max_depth, min_child_weight, subsample) {
  dtrain <- xgb.DMatrix(gb__train, label=y_label_tr)
  Pars <- list( 
      booster = "gbtree"
    , eta = 0.01
    , max_depth = max_depth
    , min_child_weight = min_child_weight
    , subsample = subsample
    , objective = "binary:logistic"
    , eval_metric = "auc"
  )
  xgbcv <- xgb.cv(
      params = Pars
    , data = dtrain
    , nround = 100
    , folds = Folds
    , prediction = TRUE
    , showsd = TRUE
    , early_stopping_rounds = 5
    , maximize = TRUE
            , verbose = 0)
  return(
    list( 
        Score = max(xgbcv$evaluation_log$test_auc_mean)
      , nrounds = xgbcv$best_iteration
    )
  )
}

```
```{r}
set.seed(50)
bounds <- list( 
    max_depth = c(2L, 10L)
  , min_child_weight = c(1, 25)
  , subsample = c(0.25, .5)
)

optObj <- bayesOpt(
    FUN = scoringFunction
  , bounds = bounds
  , initPoints = 4
  , iters.n = 3
)
optObj$scoreSummary
print(getBestPars(optObj))
```


# Build Models

## Clustering Method

We use K-nearest neighbor (KNN) to identify clusters of patients that share similar patterns that could help us predict our target variable. 

```{r cluster method knn}
set.seed(622)
mode <- function(x){
  levels <- unique(x)
  indicies <- tabulate(match(x, levels))
  levels[which.max(indicies)]
}
# Clean up training data
training_factors <- training %>% 
  dplyr::select(-Age, -`ADHD Total`, `MD TOTAL`) 
training_factors <- data.frame(lapply(training_factors, as.factor))
train_knn <- training_factors %>% 
  mutate(across(everything(), ~replace_na(., mode(.))))
mode(train_knn$Psych.meds.) 
train_knn$Psych.meds.[which(is.na(train_knn$Psych.meds.))] <- 0
sum(is.na(train_knn$Psych.meds.))

# Clean up testing data
testing_factors <- testing %>% 
  dplyr::select(-Age, -`ADHD Total`, `MD TOTAL`) 
testing_factors <- data.frame(lapply(testing_factors, as.factor))
test_knn <- testing_factors %>% 
  mutate(across(everything(), ~replace_na(., mode(.))))
mode(test_knn$Psych.meds.) 
test_knn$Psych.meds.[which(is.na(test_knn$Psych.meds.))] <- 0
sum(is.na(test_knn$Psych.meds.))

# Train KNN model
train.knn <- (train_knn[, names(train_knn) != "Suicide"])
prep <- preProcess(x = train.knn, method = c("center", "scale"))
cl <- trainControl(method="repeatedcv", repeats = 5) 
knn_model <- train(Suicide ~ ., data = train_knn, 
                method = "knn", 
                trControl = cl, 
                preProcess = c("center","scale"), 
                tuneLength = 20)
knn_model 
```


```{r}
# Evaluate Model
plot(knn_model)
knn_predict <- predict(knn_model, newdata = test_knn)
mean(knn_predict == test_knn$Suicide) # accuracy
conf.mat.knn <- confusionMatrix(knn_predict, test_knn$Suicide)
accuracy <- round(conf.mat.knn$overall[[1]], 3)*100
conf.mat.knn
```

Our KNN model accuracy comes out to `r round(conf.mat.knn$overall[[1]], 3)*100`%


## Support Vector Machine

The objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space (N being the number of features) that classifies the data points. Hyperplanes are decision boundaries to classify the data points. Data points that falls on either side of the hyperplane can be qualified for different classes. Support vectors are data points that are closer to the hyperplane and effect the position and orientation of the hyperplane. Using these support vectors, we do maximize the margin of the classifier.

There are number of R packages available to implement SVM. The train function can be used for SVM using methods as svmRadial, svmLinear and svmPoly that fit different kernels.

```{r}
# partitioning for train and test
partition <- createDataPartition(adhd_transformed$Suicide, p=0.75, list = FALSE)
training <- adhd_transformed[partition,]
testing <- adhd_transformed[-partition,]
```


```{r}
set.seed(622)

# fit with svmLinear
svm_lin_fit <- train(Suicide ~ ., 
                data = training, 
                method = "svmLinear",
                preProcess = c("center","scale"),
                tuneLength = 5,
                trControl = trainControl(method = "cv"))

pred_lin_suicide <- predict(svm_lin_fit, testing)
cm_lin <- confusionMatrix(testing$Suicide, pred_lin_suicide)

# fit with svmRadial
svm_rad_fit <- train(Suicide ~ ., 
                data = training, 
                method = "svmRadial",
                preProcess = c("center","scale"),
                tuneLength = 5,
                trControl = trainControl(method = "cv"))

pred_rad_suicide <- predict(svm_rad_fit, testing)
cm_rad <- confusionMatrix(testing$Suicide, pred_rad_suicide)

# fit with svmPoly
svm_poly_fit <- train(Suicide ~ ., 
                data = training, 
                method = "svmPoly",
                preProcess = c("center","scale"),
                tuneLength = 5,
                trControl = trainControl(method = "cv"))

pred_poly_suicide <- predict(svm_poly_fit, testing)
cm_poly <- confusionMatrix(testing$Suicide, pred_poly_suicide)

```


```{r}
#Compare 3 models:
svm_resamps <- resamples(list(Linear = svm_lin_fit, Radial = svm_rad_fit, Poly = svm_poly_fit))
summary(svm_resamps)
```


## Gradient Boosted 

We use the information from the above function to fit our final model, make preidctions, and evalute results.
```{r}
dtrain <- xgb.DMatrix(gb__train, label=y_label_tr)
dtest <- xgb.DMatrix(gb_test, label=y_label_test)
xgb <- xgb.train(
      params = list( 
                  booster = "gbtree"
                , eta = 0.01
                , max_depth = 10
                , min_child_weight = 1
                , subsample = .5
                , objective = "binary:logistic"
                , eval_metric = "auc"
              )
    , data = dtrain
    , nround = 100
    , maximize = TRUE
            , verbose = 0)

xgbpred <- predict(xgb,dtest)
xgbpred <- ifelse (xgbpred > 0.5,1,0)
y_label_test <- as.numeric(y_label_test)
confusionMatrix(table(xgbpred, y_label_test))
```


# Model Performance



# Conclusion


# References

https://towardsdatascience.com/what-is-the-difference-between-pca-and-factor-analysis-5362ef6fa6f9

https://scholarworks.umass.edu/cgi/viewcontent.cgi?article=1226&context=pare

# Code Appendix

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}

```

















